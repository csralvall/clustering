{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "clustering.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/csralvall/clustering/blob/main/clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccd3eb08"
      },
      "source": [
        "# Clustering\n",
        "\n",
        "> Trabajo perteneciente a la c√°tedra \"Text Mining\" de Laura Alonso Alemany - FaMAF UNC. 2021\n",
        "\n",
        "- _Corpus:_ compendio de noticias de \"La Voz del Interior\"\n",
        "- _Referencias:_ \n",
        "    - [word_clustering](https://github.com/danibosch/word_clustering) \n",
        "    de [@danibosch](https://github.com/danibosch)\n",
        "    - [textmining-clustering](https://github.com/facumolina/textmining-clustering) \n",
        "    de [@facumolina](https://github.com/facumolina/textmining-clustering)\n",
        "\n",
        "## Etapas:\n",
        "\n",
        "#### 1. Preprocesamiento:\n",
        "   1. [Limpieza del dataset](#regex_clean):\n",
        "   \n",
        "       Se quitan simbolos no alfanumericos mediante regex.\n",
        "       \n",
        "       \n",
        "   2. [Procesamiento con el pipeline de spacy](#spacy_pipeline):\n",
        "   \n",
        "       El pipeline de spacy ejecuta las siguientes tareas:\n",
        "       - Segmentacion en tokens.\n",
        "       - Vectorizacion.\n",
        "       - Analisis morfologico (genero, numero, etc.).\n",
        "       - Analisis de dependencias.\n",
        "       - Clasificacion por reglas.\n",
        "       - Lematizacion.\n",
        "       - Clasificacion de entidades.\n",
        "       - Conteo de frecuencias de lemas ([agregado](#lemma_freq_counter)).\n",
        "       \n",
        "       \n",
        "   3. [Filtrado](#filtering):\n",
        "   \n",
        "      Se busca un subconjunto de tokens\n",
        "      que cumplan las siguientes caracteristicas:\n",
        "       - no contiene caracteres no alfabeticos\n",
        "       - no es \"stop word\"\n",
        "       - no expresa un numero\n",
        "       \n",
        "       \n",
        "   4. [Generacion de **features**](#feature_generation):\n",
        "   \n",
        "      Por cada palabra del conjunto procesado en el paso anterior \n",
        "      generar un conjunto de features de acuerdo con:\n",
        "       - el tipo de palabra.\n",
        "       - su funcion sintactica.\n",
        "       - su coocurrencia con palabras en su misma oracion (contexto).\n",
        "       - el tipo de entidad que es, en el caso que lo sea.\n",
        "       - la dependencia con su predecesor sintactico en el arbol de dependencias.\n",
        "       \n",
        "       \n",
        "   5. [Separacion de palabras y features](#key_feature_division).\n",
        "   6. [Vectorizacion de features](#feature_vectorization).\n",
        "   7. [Normalizacion de features](#feature_normalization).\n",
        "    \n",
        "\n",
        "#### 2. [Clusterizacion](#clustering):\n",
        "   1. Eleccion del numero de clusters.\n",
        "   2. Seleccion del estado aleatorio para obtener resultados deterministicos.\n",
        "   3. Instanciacion del algoritmo KMeans para obtener los clusters.\n",
        "    \n",
        "#### 3. Resultados:\n",
        "   1. [Conteo de palabras por cluster](#summary).\n",
        "   2. [Listado de clusters con la siguiente informacion](#results):\n",
        "      - Numero de cluster.\n",
        "      - Features mas determinantes para ese cluster.\n",
        "      - Palabras dentro del cluster."
      ],
      "id": "ccd3eb08"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D78alZmcm8MX",
        "outputId": "daa7b0a0-1432-4e9a-e301-89d6e4c30cd8"
      },
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "!pip install -U spacy sklearn numpy spacy nltk gensim"
      ],
      "id": "D78alZmcm8MX",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (58.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (0.37.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (3.1.3)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.6.3)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (4.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.62.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.0)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (58.2.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.10)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.0.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk) (2019.12.20)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.6.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycbSRRGstRzV",
        "outputId": "ee9bb6c8-c968-4968-8984-796dc75354d2"
      },
      "source": [
        "!python -m spacy download es_core_news_md"
      ],
      "id": "ycbSRRGstRzV",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting es-core-news-md==3.1.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_md-3.1.0/es_core_news_md-3.1.0-py3-none-any.whl (42.7 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42.7 MB 48 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.2.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from es-core-news-md==3.1.0) (3.1.3)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->es-core-news-md==3.1.0) (3.7.4.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->es-core-news-md==3.1.0) (0.4.1)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->es-core-news-md==3.1.0) (0.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->es-core-news-md==3.1.0) (3.0.8)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->es-core-news-md==3.1.0) (2.0.5)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->es-core-news-md==3.1.0) (8.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->es-core-news-md==3.1.0) (3.0.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->es-core-news-md==3.1.0) (2.0.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->es-core-news-md==3.1.0) (2.23.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->es-core-news-md==3.1.0) (1.8.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->es-core-news-md==3.1.0) (0.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->es-core-news-md==3.1.0) (2.11.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->es-core-news-md==3.1.0) (1.21.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->es-core-news-md==3.1.0) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->es-core-news-md==3.1.0) (0.8.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->es-core-news-md==3.1.0) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->es-core-news-md==3.1.0) (21.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->es-core-news-md==3.1.0) (58.2.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.2.0,>=3.1.0->es-core-news-md==3.1.0) (2.4.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.2.0,>=3.1.0->es-core-news-md==3.1.0) (3.6.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->es-core-news-md==3.1.0) (2.4.7)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->es-core-news-md==3.1.0) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->es-core-news-md==3.1.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->es-core-news-md==3.1.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->es-core-news-md==3.1.0) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->es-core-news-md==3.1.0) (3.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.2.0,>=3.1.0->es-core-news-md==3.1.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.2.0,>=3.1.0->es-core-news-md==3.1.0) (2.0.1)\n",
            "Installing collected packages: es-core-news-md\n",
            "Successfully installed es-core-news-md-3.1.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e3bde19"
      },
      "source": [
        "from functools import reduce\n",
        "from gensim.models import Word2Vec\n",
        "import multiprocessing\n",
        "from nltk.cluster import kmeans, cosine_distance\n",
        "import numpy as np\n",
        "import re\n",
        "import spacy\n",
        "from spacy.tokens import Token\n",
        "from spacy.language import Language\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.preprocessing import normalize\n",
        "from collections import Counter"
      ],
      "id": "1e3bde19",
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dac925f"
      },
      "source": [
        "# Preprocessing"
      ],
      "id": "8dac925f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16b9e394"
      },
      "source": [
        "# load trained model from spacy\n",
        "nlp = spacy.load(\"es_core_news_md\")"
      ],
      "id": "16b9e394",
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "599677ed"
      },
      "source": [
        "<a id=\"lemma_freq_counter\"></a>"
      ],
      "id": "599677ed"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54b2d932"
      },
      "source": [
        "@Language.component(\"lemma_counter\")\n",
        "def lemma_counter(doc):\n",
        "    '''\n",
        "    custom component to count lemma frecuency\n",
        "    '''\n",
        "    lemmas = [token.lemma_ for token in doc]\n",
        "    lemmas_count = Counter(lemmas)\n",
        "    get_lemma_count = lambda x: lemmas_count[x.lemma_]\n",
        "    Token.set_extension(\"lemma_count\", getter = get_lemma_count)\n",
        "    return doc"
      ],
      "id": "54b2d932",
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eea4b537",
        "outputId": "86602604-86e1-40ee-b1fb-8400eb15a9bf"
      },
      "source": [
        "# add custom component to pipeline\n",
        "nlp.add_pipe(\"lemma_counter\", name=\"lemma_freq_counter\", last=True)"
      ],
      "id": "eea4b537",
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.lemma_counter>"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a93f24ce"
      },
      "source": [
        "<a id=\"regex_clean\"></a>"
      ],
      "id": "a93f24ce"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MG93IIp0oiyl",
        "outputId": "ea154169-4d19-4b07-8b44-4571fd453b96"
      },
      "source": [
        "!wget https://cs.famaf.unc.edu.ar/~laura/corpus/lavoztextodump.txt.tar.gz\n",
        "!tar -xvf lavoztextodump.txt.tar.gz"
      ],
      "id": "MG93IIp0oiyl",
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-06 18:40:37--  https://cs.famaf.unc.edu.ar/~laura/corpus/lavoztextodump.txt.tar.gz\n",
            "Resolving cs.famaf.unc.edu.ar (cs.famaf.unc.edu.ar)... 200.16.17.55\n",
            "Connecting to cs.famaf.unc.edu.ar (cs.famaf.unc.edu.ar)|200.16.17.55|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12136935 (12M) [application/x-gzip]\n",
            "Saving to: ‚Äòlavoztextodump.txt.tar.gz.2‚Äô\n",
            "\n",
            "lavoztextodump.txt. 100%[===================>]  11.57M  7.76MB/s    in 1.5s    \n",
            "\n",
            "2021-10-06 18:40:39 (7.76 MB/s) - ‚Äòlavoztextodump.txt.tar.gz.2‚Äô saved [12136935/12136935]\n",
            "\n",
            "lavoztextodump.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33bfda27"
      },
      "source": [
        "# open dataset\n",
        "filename = \"lavoztextodump.txt\"\n",
        "text_file = open(filename, \"r\")\n",
        "# load in memory\n",
        "dataset = text_file.read()\n",
        "#clean data\n",
        "dataset = re.sub('&#[0-9]{0,5}.', '', dataset)\n",
        "# close file\n",
        "text_file.close()"
      ],
      "id": "33bfda27",
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc5ad5f5"
      },
      "source": [
        "<a id=\"spacy_pipeline\"></a>"
      ],
      "id": "fc5ad5f5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2161e472",
        "outputId": "46617e46-02e4-46ed-9ac2-05245e91ba99"
      },
      "source": [
        "# show pipeline steps\n",
        "nlp.pipe_names"
      ],
      "id": "2161e472",
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec',\n",
              " 'morphologizer',\n",
              " 'parser',\n",
              " 'attribute_ruler',\n",
              " 'lemmatizer',\n",
              " 'ner',\n",
              " 'lemma_freq_counter']"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20b2db9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "d61edbfc-6ae9-4440-c1d2-0f691a6ac6d3"
      },
      "source": [
        "# process dataset with spacy pipeline\n",
        "# dataset is pruned to the maximum amount possible in a laptop or desk computer\n",
        "doc = nlp(dataset[:1000000])"
      ],
      "id": "20b2db9a",
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-117-4985ef189d30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# process dataset with spacy pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# dataset is pruned to the maximum amount possible in a laptop or desk computer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1002\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE109\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                 \u001b[0merror_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1005\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mraise_error\u001b[0;34m(proc_name, proc, docs, e)\u001b[0m\n\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1535\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    997\u001b[0m                 \u001b[0merror_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_error_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m                 \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                 \u001b[0;31m# This typically happens if a component is not initialized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-112-fcecb11a37b9>\u001b[0m in \u001b[0;36mlemma_counter\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlemmas_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mget_lemma_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlemmas_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mToken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lemma_count\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_lemma_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/tokens/token.pyx\u001b[0m in \u001b[0;36mspacy.tokens.token.Token.set_extension\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E090] Extension 'lemma_count' already exists on Token. To overwrite the existing extension, set `force=True` on `Token.set_extension`."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc160094"
      },
      "source": [
        "<a id=\"filtering\"></a>"
      ],
      "id": "cc160094"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1bae64b"
      },
      "source": [
        "# function designed to refine dataset processing\n",
        "def filter_tokens(doc: spacy.tokens.doc.Doc) -> [spacy.tokens.token.Token]:\n",
        "    def is_target_token(token: spacy.tokens.token.Token) -> bool:\n",
        "        return token.is_alpha and not token.is_stop and len(token.sent) > 10\n",
        "    \n",
        "    return filter(is_target_token, doc)"
      ],
      "id": "b1bae64b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b059a8e"
      },
      "source": [
        "# more dataset preprocessing\n",
        "tokens = filter_tokens(doc)"
      ],
      "id": "5b059a8e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4e65182"
      },
      "source": [
        "<a id=\"feature_generation\"></a>"
      ],
      "id": "d4e65182"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00021a43"
      },
      "source": [
        "def feature_generator(\n",
        "    tokens: [spacy.tokens.token.Token],\n",
        "    *, # all remaining arguments should be named\n",
        "    token_threshold: int,\n",
        "    context_threshold: int,\n",
        ") -> dict:\n",
        "    dicc = {}\n",
        "    for token in tokens:\n",
        "        lema = token.lemma_\n",
        "        # reduce dimensionality: discard words with not enough examples\n",
        "        if token._.lemma_count < token_threshold:\n",
        "            continue\n",
        "        # check if some form of this word has been processed before\n",
        "        # or create new dict\n",
        "        features = dicc.get(lema, {})\n",
        "\n",
        "        # create feature based on part of speech for this token\n",
        "        pos = \"POS_\" + token.pos_\n",
        "        # increment feature count\n",
        "        features[pos] = features.get(pos, 0) + 1\n",
        "    \n",
        "        # create feature based on syntactic dependency of token\n",
        "        dep = \"DEP__\" + token.dep_\n",
        "        # increment feature count\n",
        "        features[dep] = features.get(dep, 0) + 1\n",
        "            \n",
        "        # insert context based feature using co-frequence in sentences\n",
        "        for word in token.sent:\n",
        "            if word == token:\n",
        "                continue\n",
        "            # reduce dimensionality: drop unfrequent words\n",
        "            if word._.lemma_count > context_threshold:\n",
        "                if word.like_num:\n",
        "                    context = \"NUM\"\n",
        "                else:\n",
        "                    context = word.lemma_\n",
        "\n",
        "                # increment feature count\n",
        "                features[context] = features.get(context, 0) + 1\n",
        "                \n",
        "        # create feature based on type of entity\n",
        "        if token.ent_type > 0:\n",
        "            ent = \"ENT_\" + token.ent_type_\n",
        "            # increment feature count\n",
        "            features[ent] = features.get(ent, 0) + 1\n",
        "\n",
        "        # create feature based on parent-child dependency\n",
        "        tripla = \"TRIPLA__\" + token.lemma_ + \"__\" + token.dep_ + \"__\" + token.head.lemma_\n",
        "        # increment feature count\n",
        "        features[tripla] = features.get(tripla, 0) + 1\n",
        "\n",
        "        # create feature based on parent-child dependency structure\n",
        "        tripla_struct = \"TRIPLA__\" + token.pos_ + \"__\" + token.dep_ + \"__\" + token.head.pos_\n",
        "        # increment feature count\n",
        "        features[tripla] = features.get(tripla, 0) + 1\n",
        "\n",
        "        # create feature based on ancestor-token-child structure\n",
        "        for ancestor in token.ancestors:\n",
        "          for child in token.children:\n",
        "            tripla_struct = \"ATC_\" + ancestor.pos_ + \"__\" + token.pos_ + \"__\" + child.pos_ \n",
        "            # increment feature count\n",
        "            features[tripla] = features.get(tripla, 0) + 1\n",
        "\n",
        "\n",
        "    \n",
        "        dicc[lema] = features\n",
        "    \n",
        "    return dicc"
      ],
      "id": "00021a43",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "605d95a9"
      },
      "source": [
        "# get {word: features} dictionary\n",
        "dicc = feature_generator(tokens, token_threshold=15, context_threshold=50)"
      ],
      "id": "605d95a9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f337367d"
      },
      "source": [
        "<a id=\"key_feature_division\"></a>"
      ],
      "id": "f337367d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efac1e8f"
      },
      "source": [
        "# unpack keys and features in two separated tuples to ease processing\n",
        "(keys, features) = zip(*dicc.items())"
      ],
      "id": "efac1e8f",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3416b8b2"
      },
      "source": [
        "<a id=\"feature_vectorization\"></a>"
      ],
      "id": "3416b8b2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0b59882"
      },
      "source": [
        "def vectorize_features(features: (dict)) -> np.ndarray:\n",
        "    vectorized_features = DictVectorizer(sparse=False).fit_transform(features)\n",
        "    return vectorized_features"
      ],
      "id": "b0b59882",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3506332c"
      },
      "source": [
        "# use sklearn dict vectorizer to create arrays from each set of features\n",
        "vectorized_features = vectorize_features(features)"
      ],
      "id": "3506332c",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86c1115f"
      },
      "source": [
        "<a id=\"feature_normalization\"></a>"
      ],
      "id": "86c1115f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30753f47"
      },
      "source": [
        "def reduce_matrix(matrix: np.ndarray, *, variance_treshold: float):\n",
        "    print(f'INPUT SHAPE: {matrix.shape}')\n",
        "    # reduce all vectors to [0, 1] space\n",
        "    normalized_matrix = normalize(matrix, axis=1)\n",
        "    # compute variances in each row\n",
        "    matrix_variances = np.var(matrix, axis=0)\n",
        "    # create mask for features with high correlation (low variance)\n",
        "    bool_mask = np.where(matrix_variances < variance_treshold)\n",
        "    # filter features with high correlation (variance under treshold)\n",
        "    raked_matrix = np.delete(normalized_matrix, bool_mask, axis=1)\n",
        "    print(f'OUTPUT SHAPE: {raked_matrix.shape}')\n",
        "    return raked_matrix"
      ],
      "id": "30753f47",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47ea87b6",
        "outputId": "9e24d39d-dc91-439c-c20c-79c0fe3351bf"
      },
      "source": [
        "reduced_matrix = reduce_matrix(vectorized_features, variance_treshold=0.001)"
      ],
      "id": "47ea87b6",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INPUT SHAPE: (1664, 39633)\n",
            "OUTPUT SHAPE: (1664, 39632)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ce93b06"
      },
      "source": [
        "<a id=\"clustering\"></a>"
      ],
      "id": "1ce93b06"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e419e0d3"
      },
      "source": [
        "# Clustering"
      ],
      "id": "e419e0d3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15812dc5"
      },
      "source": [
        "def generate_clusters(\n",
        "    matrix: np.ndarray,\n",
        "    n_clusters: int\n",
        ") -> KMeans:\n",
        "    # generate word clusters using the KMeans algorithm.\n",
        "    print(\"\\nClustering started\")\n",
        "    # Instantiate KMeans clusterer for n_clusters\n",
        "    km_model = KMeans(n_clusters=n_clusters, random_state=3)\n",
        "    # create clusters\n",
        "    km_model.fit(matrix)\n",
        "    print(\"Clustering finished\")\n",
        "    return km_model"
      ],
      "id": "15812dc5",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9772c06a",
        "outputId": "22de9d9f-215a-4588-9b70-7c8044b1b200"
      },
      "source": [
        "clusters = generate_clusters(reduced_matrix, 150)"
      ],
      "id": "9772c06a",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Clustering started\n",
            "Clustering finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75b63800"
      },
      "source": [
        "# Results"
      ],
      "id": "75b63800"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92e4aa91"
      },
      "source": [
        "<a id=\"summary\"></a>"
      ],
      "id": "92e4aa91"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e022323"
      },
      "source": [
        "def display_summary(clusters: KMeans):\n",
        "    cluster_count = Counter(sorted(clusters.labels_))\n",
        "    for cluster in cluster_count:\n",
        "        print (\"Cluster#\", cluster,\" - Total words:\", cluster_count[cluster])"
      ],
      "id": "6e022323",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27cb66af"
      },
      "source": [
        "# show number of words captured by each cluster\n",
        "display_summary(clusters)"
      ],
      "id": "27cb66af",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19f8b6c4"
      },
      "source": [
        "<a id=\"results\"></a>"
      ],
      "id": "19f8b6c4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59293775"
      },
      "source": [
        "def display_clusters(clusters: KMeans, keys: (str)):\n",
        "    cluster_count = Counter(sorted(clusters.labels_))\n",
        "    print(\"Top words per cluster:\\n\")\n",
        "    #sort cluster centers by proximity to centroid\n",
        "    order_centroids = clusters.cluster_centers_.argsort()[:, ::-1] \n",
        "    \n",
        "    for cluster_idx in cluster_count:\n",
        "        print(f\"Cluster {cluster_idx} - Total words: {cluster_count[cluster_idx]}\")\n",
        "        print(\"\\n\\nWords:\", end='')\n",
        "        # get words inside each cluster\n",
        "        cluster_words = np.where(clusters.labels_ == cluster_idx)[0]\n",
        "        # print all words inside cluster\n",
        "        for idx in cluster_words:\n",
        "            print(f\" {keys[idx]}\", end=\",\")\n",
        "        print(\"\\n\\n\")"
      ],
      "id": "59293775",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnmWaWfUw8lV"
      },
      "source": [
        "def display_cluster_features(clusters: KMeans, features: (dict)):\n",
        "    cluster_count = Counter(sorted(clusters.labels_))\n",
        "    print(\"Most decisive features for each cluster:\\n\")\n",
        "    #sort cluster centers by proximity to centroid\n",
        "    order_centroids = clusters.cluster_centers_.argsort()[:, ::-1] \n",
        "    \n",
        "    # flatten features dict\n",
        "    flat_features = reduce(lambda x, y: {**x, **y}, features, {})\n",
        "    # get feature keys\n",
        "    feature_keys = list(flat_features.keys())\n",
        "\n",
        "    # iterate over each cluster\n",
        "    for cluster_idx in cluster_count:\n",
        "        print(f\"Cluster {cluster_idx} - Total words: {cluster_count[cluster_idx]}\")\n",
        "        print(\"Frequent terms:\", end='')\n",
        "        # print most determinant features for this cluster\n",
        "        for ind in order_centroids[cluster_idx, :10]:\n",
        "            print(f' {feature_keys[ind]}', end=',')\n",
        "\n",
        "        print(\"\\n\\n\")"
      ],
      "id": "DnmWaWfUw8lV",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "276e2e4a"
      },
      "source": [
        "display_clusters(clusters)"
      ],
      "id": "276e2e4a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35605b5a"
      },
      "source": [
        "def search_word_cluster(clusters: KMeans, corpus: (str), searched_word: str) -> [str]:\n",
        "    clusts = clusters.labels_\n",
        "    search_idx = corpus.index(searched_word)\n",
        "    return [word for idx, word in enumerate(corpus, 0) if clusts[search_idx] == clusts[idx]]"
      ],
      "id": "35605b5a",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8ef2977"
      },
      "source": [
        "search_word_cluster(clusters, keys, 'viernes')"
      ],
      "id": "f8ef2977",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UydcEoy8LhWd"
      },
      "source": [
        "search_word_cluster(clusters, keys, 'Brasil')"
      ],
      "id": "UydcEoy8LhWd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7HTLzayLqr4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdfc327c-6750-421d-fa8f-a59a03164ac6"
      },
      "source": [
        "search_word_cluster(clusters, keys, 'Sota')"
      ],
      "id": "r7HTLzayLqr4",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Sota']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dtIB2HPL0NW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40276164-4a14-48fe-e12d-8473dbbae843"
      },
      "source": [
        "search_word_cluster(clusters, keys, 'Facultad')"
      ],
      "id": "5dtIB2HPL0NW",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Facultad']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGzvrHqZMCpi"
      },
      "source": [
        "search_word_cluster(clusters, keys, 'colegio')"
      ],
      "id": "QGzvrHqZMCpi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "478a437a"
      },
      "source": [
        "def in_same_cluster(clusters: KMeans, corpus: (str), words: [str]) -> bool:\n",
        "    clusts = clusters.labels_\n",
        "    word_clusters = map(lambda w: clusts[corpus.index(w)], words)\n",
        "    number_of_clusters = len(set(word_clusters))\n",
        "    return number_of_clusters <= 1"
      ],
      "id": "478a437a",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "940c226a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abc1dca3-0e89-4244-cd6a-61447e43e097"
      },
      "source": [
        "in_same_cluster(clusters, keys, ['lunes', 'jueves', 'viernes', 'domingo'])"
      ],
      "id": "940c226a",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ac17089"
      },
      "source": [
        "in_same_cluster(clusters, keys, ['lunes', 'martes'])"
      ],
      "id": "2ac17089",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUoVsV-FNFO2"
      },
      "source": [
        "# Clustering with word embeddings"
      ],
      "id": "MUoVsV-FNFO2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqd1-4CB53NW"
      },
      "source": [
        "# load trained model from spacy\n",
        "nlp = spacy.load(\"es_core_news_md\")"
      ],
      "id": "fqd1-4CB53NW",
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfFB2rE-5Yci",
        "outputId": "fcb669dc-c621-4c79-8988-cefeb25de7ac"
      },
      "source": [
        "# show pipeline steps\n",
        "nlp.pipe_names"
      ],
      "id": "FfFB2rE-5Yci",
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec', 'morphologizer', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0_6F-IK9mAx"
      },
      "source": [
        "# process dataset with spacy pipeline\n",
        "# dataset is pruned to the maximum amount possible in a laptop or desk computer\n",
        "# disable ner and parser for speed\n",
        "with nlp.select_pipes(disable=[\"ner\"]):\n",
        "  doc = nlp(dataset[:1000000])"
      ],
      "id": "R0_6F-IK9mAx",
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5KBGzMOLKRI"
      },
      "source": [
        "# function designed to refine dataset processing\n",
        "def filter_tokens_in_sent(sent: spacy.tokens.span.Span) -> [spacy.tokens.token.Token]:\n",
        "    def is_target_token(token: spacy.tokens.token.Token) -> bool:\n",
        "        return token.is_alpha and not token.is_stop\n",
        "    \n",
        "    return filter(is_target_token, sent)"
      ],
      "id": "l5KBGzMOLKRI",
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_l89e-fM0N8"
      },
      "source": [
        "def lemmatize(token: spacy.tokens.token.Token) -> str:\n",
        "  return token.lemma_"
      ],
      "id": "G_l89e-fM0N8",
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8M1uegxuMJL3"
      },
      "source": [
        "def filter_sents(doc: spacy.tokens.doc.Doc) -> [[str]]:\n",
        "  sents = []\n",
        "  for sent in doc.sents:\n",
        "      sent_tokens = filter_tokens_in_sent(sent)\n",
        "      sents.append(list(map(lemmatize, sent_tokens)))\n",
        "  return sents"
      ],
      "id": "8M1uegxuMJL3",
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ROI1NiyO_Gh"
      },
      "source": [
        "def generate_embedding(sentences: [[str]]) -> ([str], np.ndarray):\n",
        "  # Count the number of cores in a computer\n",
        "  cores = multiprocessing.cpu_count()\n",
        "  w2v_model = Word2Vec(\n",
        "                     min_count=20,\n",
        "                     window=2,\n",
        "                     #size=300,\n",
        "                     sample=6e-5, \n",
        "                     alpha=0.03, \n",
        "                     min_alpha=0.0007, \n",
        "                     negative=20,\n",
        "                     workers=cores-1)\n",
        "\n",
        "  w2v_model.build_vocab(sentences, progress_per=10000)\n",
        "\n",
        "  w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
        "\n",
        "  words = w2v_model.wv.index_to_key\n",
        "  normed_vectors = w2v_model.wv.get_normed_vectors()\n",
        "\n",
        "  return (words, normed_vectors)"
      ],
      "id": "7ROI1NiyO_Gh",
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3w2MjTVhNIPr"
      },
      "source": [
        "words, normed_vectors = generate_embedding(filter_sents(doc))"
      ],
      "id": "3w2MjTVhNIPr",
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6LHQPxhQ17T"
      },
      "source": [
        "clusters = generate_clusters(normed_vectors, 45)"
      ],
      "id": "c6LHQPxhQ17T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDeXcS8KRRVp"
      },
      "source": [
        "display_summary(clusters)"
      ],
      "id": "UDeXcS8KRRVp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yV-RMD_OTP9m"
      },
      "source": [
        "display_clusters(clusters, words)"
      ],
      "id": "yV-RMD_OTP9m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PigbA_0iTRv3"
      },
      "source": [
        ""
      ],
      "id": "PigbA_0iTRv3",
      "execution_count": null,
      "outputs": []
    }
  ]
}