{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccd3eb08",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "> Trabajo perteneciente a la cátedra \"Text Mining\" de Laura Alonso Alemany - FaMAF UNC. 2021\n",
    "\n",
    "- _Corpus:_ compendio de noticias de \"La Voz del Interior\"\n",
    "- _Referencias:_ \n",
    "    - [word_clustering](https://github.com/danibosch/word_clustering) \n",
    "    de [@danibosch](https://github.com/danibosch)\n",
    "    - [textmining-clustering](https://github.com/facumolina/textmining-clustering) \n",
    "    de [@facumolina](https://github.com/facumolina/textmining-clustering)\n",
    "\n",
    "## Etapas:\n",
    "\n",
    "#### 1. Preprocesamiento:\n",
    "   1. [Limpieza del dataset](#regex_clean):\n",
    "   \n",
    "       Se quitan simbolos no alfanumericos mediante regex.\n",
    "       \n",
    "       \n",
    "   2. [Procesamiento con el pipeline de spacy](#spacy_pipeline):\n",
    "   \n",
    "       El pipeline de spacy ejecuta las siguientes tareas:\n",
    "       - Segmentacion en tokens.\n",
    "       - Vectorizacion.\n",
    "       - Analisis morfologico (genero, numero, etc.).\n",
    "       - Analisis de dependencias.\n",
    "       - Clasificacion por reglas.\n",
    "       - Lematizacion.\n",
    "       - Clasificacion de entidades.\n",
    "       - Conteo de frecuencias de lemas ([agregado](#lemma_freq_counter)).\n",
    "       \n",
    "       \n",
    "   3. [Filtrado](#filtering):\n",
    "   \n",
    "      Se busca un subconjunto de tokens\n",
    "      que cumplan las siguientes caracteristicas:\n",
    "       - no contiene caracteres no alfabeticos\n",
    "       - no es \"stop word\"\n",
    "       - no expresa un numero\n",
    "       \n",
    "       \n",
    "   4. [Generacion de **features**](#feature_generation):\n",
    "   \n",
    "      Por cada palabra del conjunto procesado en el paso anterior \n",
    "      generar un conjunto de features de acuerdo con:\n",
    "       - el tipo de palabra.\n",
    "       - su funcion sintactica.\n",
    "       - su coocurrencia con palabras en su misma oracion (contexto).\n",
    "       - el tipo de entidad que es, en el caso que lo sea.\n",
    "       - la dependencia con su predecesor sintactico en el arbol de dependencias.\n",
    "       \n",
    "       \n",
    "   5. [Separacion de palabras y features](#key_feature_division).\n",
    "   6. [Vectorizacion de features](#feature_vectorization).\n",
    "   7. [Normalizacion de features](#feature_normalization).\n",
    "    \n",
    "\n",
    "#### 2. [Clusterizacion](#clustering):\n",
    "   1. Eleccion del numero de clusters.\n",
    "   2. Seleccion del estado aleatorio para obtener resultados deterministicos.\n",
    "   3. Instanciacion del algoritmo KMeans para obtener los clusters.\n",
    "    \n",
    "#### 3. Resultados:\n",
    "   1. [Conteo de palabras por cluster](#summary).\n",
    "   2. [Listado de clusters con la siguiente informacion](#results):\n",
    "      - Numero de cluster.\n",
    "      - Features mas determinantes para ese cluster.\n",
    "      - Palabras dentro del cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e3bde19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "from spacy.language import Language\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dac925f",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16b9e394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained model from spacy\n",
    "nlp = spacy.load(\"es_core_news_sm\", exclude=['vectors'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599677ed",
   "metadata": {},
   "source": [
    "<a id=\"lemma_freq_counter\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54b2d932",
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"lemma_counter\")\n",
    "def lemma_counter(doc):\n",
    "    '''\n",
    "    custom component to count lemma frecuency\n",
    "    '''\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    lemmas_count = Counter(lemmas)\n",
    "    get_lemma_count = lambda x: lemmas_count[x.lemma_]\n",
    "    Token.set_extension(\"lemma_count\", getter = get_lemma_count)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eea4b537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.lemma_counter(doc)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add custom component to pipeline\n",
    "nlp.add_pipe(\"lemma_counter\", name=\"lemma_freq_counter\", last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93f24ce",
   "metadata": {},
   "source": [
    "<a id=\"regex_clean\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33bfda27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open dataset\n",
    "filename = \"lavoztextodump.txt\"\n",
    "text_file = open(filename, \"r\")\n",
    "# load in memory\n",
    "dataset = text_file.read()\n",
    "#clean data\n",
    "dataset = re.sub('&#[0-9]{0,5}.', '', dataset)\n",
    "# close file\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5ad5f5",
   "metadata": {},
   "source": [
    "<a id=\"spacy_pipeline\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2161e472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec',\n",
       " 'morphologizer',\n",
       " 'parser',\n",
       " 'attribute_ruler',\n",
       " 'lemmatizer',\n",
       " 'ner',\n",
       " 'lemma_freq_counter']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show pipeline steps\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20b2db9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process dataset with spacy pipeline\n",
    "# dataset is pruned to the maximum amount possible in a laptop or desk computer\n",
    "doc = nlp(dataset[:1000000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc160094",
   "metadata": {},
   "source": [
    "<a id=\"filtering\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1bae64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function designed to refine dataset processing\n",
    "def filter_tokens(doc: spacy.tokens.doc.Doc) -> [spacy.tokens.token.Token]:\n",
    "    def is_target_token(token: spacy.tokens.token.Token) -> bool:\n",
    "        return token.is_alpha and not token.like_num and not token.is_stop\n",
    "    \n",
    "    return filter(is_target_token, doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b059a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more dataset preprocessing\n",
    "tokens = filter_tokens(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e65182",
   "metadata": {},
   "source": [
    "<a id=\"feature_generation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00021a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_generator(\n",
    "    tokens: [spacy.tokens.token.Token],\n",
    "    threshold_token: int,\n",
    "    threshold_context: int,\n",
    ") -> dict:\n",
    "    dicc = {}\n",
    "    for token in tokens:\n",
    "        lema = token.lemma_\n",
    "        features = {}\n",
    "        # reduce dimensionality: discard words with not enough examples\n",
    "        if token._.lemma_count < threshold_token:\n",
    "            continue\n",
    "        # check if some form of this word has been processed before\n",
    "        if lema in dicc:\n",
    "            features = dicc[lema]\n",
    "\n",
    "        # create feature based on part of speech for this token\n",
    "        pos = \"POS_\" + token.pos_\n",
    "        if not pos in features:\n",
    "            features.setdefault(pos, 0)\n",
    "        features[pos] += 1\n",
    "    \n",
    "        # create feature based on syntactic dependency of token\n",
    "        dep = \"DEP__\" + token.dep_\n",
    "        if not dep in features:\n",
    "            features.setdefault(dep, 0)\n",
    "        features[dep] += 1\n",
    "            \n",
    "        # insert context based feature using co-frequence in sentences\n",
    "        for word in token.sent:\n",
    "            # reduce dimensionality: drop unfrequent words\n",
    "            if word._.lemma_count > threshold_context:\n",
    "                if word.like_num:\n",
    "                    context = \"NUM\"\n",
    "                elif word.is_alpha and not word.is_stop:\n",
    "                    context = word.lemma_\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "                if not context in features:\n",
    "                    features.setdefault(context, 0)\n",
    "                features[context] += 1\n",
    "                \n",
    "        # create feature based on type of entity\n",
    "        if token.ent_type:\n",
    "            ent = \"ENT_\" + token.ent_type_\n",
    "            if not ent in features:\n",
    "                features.setdefault(ent, 0)\n",
    "            features[ent] += 1\n",
    "\n",
    "        # create feature based on parent-child dependency\n",
    "        tripla = \"TRIPLA__\" + token.lemma_ + \"__\" + token.dep_ + \"__\" + token.head.lemma_\n",
    "        if not tripla in features:\n",
    "            features.setdefault(tripla, 0)\n",
    "        features[tripla] += 1\n",
    "    \n",
    "        dicc[lema] = features\n",
    "    \n",
    "    return dicc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "605d95a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get {word: features} dictionary\n",
    "dicc = feature_generator(tokens, 100, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f337367d",
   "metadata": {},
   "source": [
    "<a id=\"key_feature_division\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efac1e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unpack keys and features in two separated tuples to ease processing\n",
    "(keys, features) = zip(*dicc.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3416b8b2",
   "metadata": {},
   "source": [
    "<a id=\"feature_vectorization\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0b59882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_features(features: (dict)) -> np.ndarray:\n",
    "    matrix = DictVectorizer(sparse=False).fit_transform(features)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3506332c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sklearn dict vectorizer to create arrays from each set of features\n",
    "matrix = vectorize_features(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c1115f",
   "metadata": {},
   "source": [
    "<a id=\"feature_normalization\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47ea87b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce all vectors to [0, 1] space\n",
    "matrix = normalize(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce93b06",
   "metadata": {},
   "source": [
    "<a id=\"clustering\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e419e0d3",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15812dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clusters(\n",
    "    matrix: np.ndarray,\n",
    "    n_clusters: int\n",
    ") -> KMeans:\n",
    "    # generate word clusters using the KMeans algorithm.\n",
    "    print(\"\\nClustering started\")\n",
    "    # Instantiate KMeans clusterer for n_clusters\n",
    "    km_model = KMeans(n_clusters=n_clusters, random_state=3)\n",
    "    # create clusters\n",
    "    km_model.fit(matrix)\n",
    "    print(\"Clustering finished\")\n",
    "    return km_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9772c06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clustering started\n",
      "Clustering finished\n"
     ]
    }
   ],
   "source": [
    "clusters = generate_clusters(matrix, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b63800",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e4aa91",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e022323",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_summary(clusters: KMeans):\n",
    "    cluster_count = Counter(sorted(clusters.labels_))\n",
    "    for cluster in cluster_count:\n",
    "        print (\"Cluster#\", cluster,\" - Total words:\", cluster_count[cluster])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27cb66af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster# 0  - Total words: 3\n",
      "Cluster# 1  - Total words: 2\n",
      "Cluster# 2  - Total words: 1\n",
      "Cluster# 3  - Total words: 2\n",
      "Cluster# 4  - Total words: 6\n",
      "Cluster# 5  - Total words: 3\n",
      "Cluster# 6  - Total words: 11\n",
      "Cluster# 7  - Total words: 6\n",
      "Cluster# 8  - Total words: 3\n",
      "Cluster# 9  - Total words: 2\n",
      "Cluster# 10  - Total words: 5\n",
      "Cluster# 11  - Total words: 1\n",
      "Cluster# 12  - Total words: 6\n",
      "Cluster# 13  - Total words: 1\n",
      "Cluster# 14  - Total words: 6\n",
      "Cluster# 15  - Total words: 1\n",
      "Cluster# 16  - Total words: 2\n",
      "Cluster# 17  - Total words: 9\n",
      "Cluster# 18  - Total words: 1\n",
      "Cluster# 19  - Total words: 4\n"
     ]
    }
   ],
   "source": [
    "# show number of words captured by each cluster\n",
    "display_summary(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f8b6c4",
   "metadata": {},
   "source": [
    "<a id=\"results\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59293775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_clusters(clusters: KMeans):\n",
    "    cluster_count = Counter(sorted(clusters.labels_))\n",
    "    print(\"Top terms and words per cluster:\\n\")\n",
    "    #sort cluster centers by proximity to centroid\n",
    "    order_centroids = clusters.cluster_centers_.argsort()[:, ::-1] \n",
    "    \n",
    "    # flatten features dict\n",
    "    flat_features = reduce(lambda x, y: x | y, features, {})\n",
    "    # get feature keys\n",
    "    feature_keys = list(flat_features.keys())\n",
    "\n",
    "    # iterate over each cluster\n",
    "    for cluster_idx in cluster_count:\n",
    "        print(f\"Cluster {cluster_idx} - Total words: {cluster_count[cluster_idx]}\")\n",
    "        print(\"Frequent terms:\", end='')\n",
    "        # print most determinant features for this cluster\n",
    "        for ind in order_centroids[cluster_idx, :10]:\n",
    "            print(f' {feature_keys[ind]}', end=',')\n",
    "\n",
    "        print(\"\\n\\nWords:\", end='')\n",
    "        # get words inside each cluster\n",
    "        cluster_words = np.where(clusters.labels_ == cluster_idx)[0]\n",
    "        # print all words inside cluster\n",
    "        for idx in cluster_words:\n",
    "            print(f\" {keys[idx]}\", end=\",\")\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "276e2e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms and words per cluster:\n",
      "\n",
      "Cluster 0 - Total words: 3\n",
      "Frequent terms: colegio, TRIPLA__a__mark__cambiar, TRIPLA__tanto__nmod__otro, TRIPLA__poder__acl__Honduras, TRIPLA__Schiaretti__nsubj__eludir, TRIPLA__ser__cop__número, TRIPLA__Schiaretti__nmod__relación, lograr, TRIPLA__medio__conj__sector, grupo,\n",
      "\n",
      "Words: haber, deber, poder,\n",
      "\n",
      "\n",
      "Cluster 1 - Total words: 2\n",
      "Frequent terms: grupo, TRIPLA__tanto__nmod__otro, TRIPLA__poder__aux__asistir, TRIPLA__tanto__det__sed, TRIPLA__poder__acl__Honduras, dejar, problema, o, TRIPLA__a__advmod__dejar, TRIPLA__a__case__medida,\n",
      "\n",
      "Words: caso, tiempo,\n",
      "\n",
      "\n",
      "Cluster 2 - Total words: 1\n",
      "Frequent terms: TRIPLA__tanto__det__tractor, TRIPLA__a__case__unidad, TRIPLA__poder__acl__Honduras, TRIPLA__tanto__nmod__otro, TRIPLA__ciento__compound__230, DEP__fixed, persona, lograr, proyecto, TRIPLA__a__case__muerte,\n",
      "\n",
      "Words: ver,\n",
      "\n",
      "\n",
      "Cluster 3 - Total words: 2\n",
      "Frequent terms: TRIPLA__a__mark__llamar, toma, Gobierno, TRIPLA__poder__acl__Honduras, año, TRIPLA__tanto__nmod__otro, TRIPLA__Schiaretti__nsubj__pasar, TRIPLA__a__case__plazo, DEP__mark, TRIPLA__a__mark__representar,\n",
      "\n",
      "Words: Juan, Schiaretti,\n",
      "\n",
      "\n",
      "Cluster 4 - Total words: 6\n",
      "Frequent terms: TRIPLA__a__case__centro, TRIPLA__a__case__circunstancia, TRIPLA__tanto__nmod__otro, TRIPLA__poder__acl__Honduras, lograr, TRIPLA__fin__obl__procesar, TRIPLA__ser__cop__necesario, TRIPLA__medio__obl__analizar, TRIPLA__primero__amod__novela, TRIPLA__primero__amod__estimación,\n",
      "\n",
      "Words: mayor, nacional, provincial, público, político, primero,\n",
      "\n",
      "\n",
      "Cluster 5 - Total words: 3\n",
      "Frequent terms: grupo, TRIPLA__medio__nmod__esquivo, TRIPLA__tanto__nmod__otro, TRIPLA__ser__nmod__uno, TRIPLA__poder__acl__Honduras, lograr, dejar, TRIPLA__poder__aux__ocultar, TRIPLA__a__advmod__dejar, TRIPLA__poder__aux__extraer,\n",
      "\n",
      "Words: millón, bien, pesos,\n",
      "\n",
      "\n",
      "Cluster 6 - Total words: 11\n",
      "Frequent terms: grupo, TRIPLA__tanto__nmod__otro, TRIPLA__poder__acl__Honduras, dejar, lograr, o, TRIPLA__a__advmod__dejar, TRIPLA__ser__cop__instrumento, TRIPLA__primero__amod__48, TRIPLA__Schiaretti__appos__gobernador,\n",
      "\n",
      "Words: presidente, país, empresa, sector, política, voto, gobernador, elección, gobierno, medio, ser,\n",
      "\n",
      "\n",
      "Cluster 7 - Total words: 6\n",
      "Frequent terms: grupo, TRIPLA__poder__acl__Honduras, TRIPLA__tanto__nmod__otro, TRIPLA__Schiaretti__nsubj__castigar, TRIPLA__tanto__det__gente, dejar, o, TRIPLA__poder__nmod__relación, TRIPLA__Schiaretti__flat__cambio, TRIPLA__poder__aux__afectar,\n",
      "\n",
      "Words: toma, colegio, estudiante, escuela, chico, parte,\n",
      "\n",
      "\n",
      "Cluster 8 - Total words: 3\n",
      "Frequent terms: educación, TRIPLA__a__case__sólo, TRIPLA__tanto__nmod__otro, TRIPLA__poder__acl__Honduras, TRIPLA__medio__conj__sector, TRIPLA__Schiaretti__obl__insistir, lograr, TRIPLA__poder__aux__extraer, TRIPLA__a__advmod__sutil, TRIPLA__poder__obj__respetar,\n",
      "\n",
      "Words: y, o, e,\n",
      "\n",
      "\n",
      "Cluster 9 - Total words: 2\n",
      "Frequent terms: llamar, TRIPLA__tanto__nmod__otro, TRIPLA__poder__obj__respetar, TRIPLA__poder__acl__Honduras, TRIPLA__medio__obl__contactar, asegurar, lograr, TRIPLA__a__advmod__dejar, TRIPLA__poder__aux__extraer, problema,\n",
      "\n",
      "Words: ciento, mil,\n",
      "\n",
      "\n",
      "Cluster 10 - Total words: 5\n",
      "Frequent terms: TRIPLA__a__mark__llamar, TRIPLA__tanto__nmod__otro, TRIPLA__poder__acl__Honduras, Luis, TRIPLA__a__advmod__sutil, dejar, obra, volver, DEP__case, conocer,\n",
      "\n",
      "Words: Gobierno, Córdoba, Argentina, San, Justicia,\n",
      "\n",
      "\n",
      "Cluster 11 - Total words: 1\n",
      "Frequent terms: TRIPLA__tanto__nmod__otro, TRIPLA__poder__acl__Honduras, grupo, TRIPLA__mil__nsubj__recorrer, TRIPLA__nacional__amod__interés, TRIPLA__fin__obl__desechar, ENT_MISC, TRIPLA__a__advmod__dejar, TRIPLA__tanto__conj__presidente, volver,\n",
      "\n",
      "Words: sobre,\n",
      "\n",
      "\n",
      "Cluster 12 - Total words: 6\n",
      "Frequent terms: grupo, TRIPLA__tanto__nmod__otro, TRIPLA__poder__acl__Honduras, lograr, TRIPLA__medio__nsubj__obligado, dejar, TRIPLA__a__advmod__dejar, TRIPLA__medio__obl__aparecer, TRIPLA__ser__nmod__contacto, TRIPLA__fin__nmod__inmigración,\n",
      "\n",
      "Words: persona, proyecto, ley, plan, obra, fin,\n",
      "\n",
      "\n",
      "Cluster 13 - Total words: 1\n",
      "Frequent terms: TRIPLA__pasado__amod__dolor, agregar, TRIPLA__tanto__nmod__otro, TRIPLA__a__mark__tener, TRIPLA__poder__acl__Honduras, lograr, dejar, TRIPLA__poder__aux__extraer, TRIPLA__tanto__det__tractor, TRIPLA__nacional__amod__coyuntura,\n",
      "\n",
      "Words: tanto,\n",
      "\n",
      "\n",
      "Cluster 14 - Total words: 6\n",
      "Frequent terms: TRIPLA__a__case__unidad, TRIPLA__tanto__nmod__otro, TRIPLA__poder__acl__Honduras, DEP__fixed, persona, TRIPLA__tanto__conj__presidente, POS_DET, TRIPLA__ser__nsubj__denunciar, TRIPLA__mil__obl__migrar, TRIPLA__medio__obj__vender,\n",
      "\n",
      "Words: seguir, tomar, parecer, tener, dar, hacer,\n",
      "\n",
      "\n",
      "Cluster 15 - Total words: 1\n",
      "Frequent terms: TRIPLA__a__case__centro, TRIPLA__ser__cop__jefe, TRIPLA__tanto__nmod__otro, TRIPLA__medio__nmod__portada, TRIPLA__colegio__obl__autoconvocado, TRIPLA__colegio__obl__declarar, TRIPLA__a__advmod__dejar, TRIPLA__a__case__circunstancia, TRIPLA__poder__aux__convertir él, TRIPLA__y__cc__desarrollar,\n",
      "\n",
      "Words: pasado,\n",
      "\n",
      "\n",
      "Cluster 16 - Total words: 2\n",
      "Frequent terms: TRIPLA__poder__acl__Honduras, TRIPLA__tanto__nmod__otro, TRIPLA__medio__nummod__mañana, saber, sentido, lograr, TRIPLA__a__case__unidad, colegio, TRIPLA__a__mark__cambiar, TRIPLA__poder__aux__extraer,\n",
      "\n",
      "Words: a, ir,\n",
      "\n",
      "\n",
      "Cluster 17 - Total words: 9\n",
      "Frequent terms: TRIPLA__a__case__unidad, TRIPLA__poder__acl__Honduras, TRIPLA__tanto__nmod__otro, DEP__fixed, lograr, persona, vivir, TRIPLA__ser__csubj__sospechar, TRIPLA__fin__obj__haber, TRIPLA__fin__obl__asistir,\n",
      "\n",
      "Words: asegurar, querer, dejar, llegar, quedar, decir, pasar, recibir, realizar,\n",
      "\n",
      "\n",
      "Cluster 18 - Total words: 1\n",
      "Frequent terms: TRIPLA__Schiaretti__nsubj__comenzar, colegio, TRIPLA__tanto__nmod__otro, objetivo, TRIPLA__poder__acl__Honduras, TRIPLA__a__mark__cambiar, lograr, TRIPLA__a__advmod__sutil, TRIPLA__a__case__unidad, DEP__fixed,\n",
      "\n",
      "Words: estar,\n",
      "\n",
      "\n",
      "Cluster 19 - Total words: 4\n",
      "Frequent terms: grupo, TRIPLA__tanto__nmod__otro, TRIPLA__poder__acl__Honduras, lograr, problema, dejar, TRIPLA__poder__aux__extraer, TRIPLA__medio__nmod__noticia, TRIPLA__mil__obj__vitoreado, TRIPLA__poder__aux__existir,\n",
      "\n",
      "Words: año, mes, ciudad, semana,\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_clusters(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ef2977",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
